{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34397159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning LogisticRegression...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for LogisticRegression: {'model__C': 1559.9453033620264, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
      "Best cross-validation score for LogisticRegression: 0.5208\n",
      "Train mean accuracy: 0.5208, Train std deviation: 0.0331\n",
      "\n",
      "Tuning RandomForestClassifier...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for RandomForestClassifier: {'model__max_depth': 30, 'model__min_samples_leaf': 8, 'model__min_samples_split': 4, 'model__n_estimators': 199}\n",
      "Best cross-validation score for RandomForestClassifier: 0.5476\n",
      "Train mean accuracy: 0.5402, Train std deviation: 0.0018\n",
      "\n",
      "Tuning SVC...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for SVC: {'model__C': 20.585494295802448, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\n",
      "Best cross-validation score for SVC: 0.5677\n",
      "Train mean accuracy: 0.5677, Train std deviation: 0.0295\n",
      "\n",
      "Tuning GradientBoostingClassifier...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for GradientBoostingClassifier: {'model__learning_rate': 0.10184977839317343, 'model__max_depth': 3, 'model__n_estimators': 149}\n",
      "Best cross-validation score for GradientBoostingClassifier: 0.5439\n",
      "Train mean accuracy: 0.5439, Train std deviation: 0.0286\n",
      "\n",
      "Tuning KNeighborsClassifier...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for KNeighborsClassifier: {'model__n_neighbors': 5, 'model__p': 2, 'model__weights': 'uniform'}\n",
      "Best cross-validation score for KNeighborsClassifier: 0.5536\n",
      "Train mean accuracy: 0.5499, Train std deviation: 0.0341\n",
      "\n",
      "Tuning DecisionTreeClassifier...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for DecisionTreeClassifier: {'model__max_depth': 20, 'model__min_samples_leaf': 7, 'model__min_samples_split': 12}\n",
      "Best cross-validation score for DecisionTreeClassifier: 0.4926\n",
      "Train mean accuracy: 0.4836, Train std deviation: 0.0184\n",
      "\n",
      "Tuning GaussianNB...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for GaussianNB: {}\n",
      "Best cross-validation score for GaussianNB: 0.4881\n",
      "Train mean accuracy: 0.4881, Train std deviation: 0.0312\n",
      "\n",
      "Tuning XGBClassifier...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:29:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:33:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:36:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:40:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBClassifier: {'model__learning_rate': 0.13349630192554332, 'model__max_depth': 4, 'model__n_estimators': 71}\n",
      "Best cross-validation score for XGBClassifier: 0.5536\n",
      "Train mean accuracy: 0.5536, Train std deviation: 0.0330\n",
      "\n",
      "Tuning AdaBoostClassifier...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for AdaBoostClassifier: {'model__learning_rate': 0.3845401188473625, 'model__n_estimators': 142}\n",
      "Best cross-validation score for AdaBoostClassifier: 0.5312\n",
      "Train mean accuracy: 0.5312, Train std deviation: 0.0301\n",
      "\n",
      "Tuning CatBoostClassifier...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for CatBoostClassifier: {'model__depth': 7, 'model__iterations': 142, 'model__learning_rate': 0.04668695797323276}\n",
      "Best cross-validation score for CatBoostClassifier: 0.5536\n",
      "Train mean accuracy: 0.5536, Train std deviation: 0.0128\n",
      "\n",
      "Evaluating models on test data...\n",
      "Classification report for LogisticRegression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.58      0.59       125\n",
      "           1       0.48      0.53      0.51       124\n",
      "           2       0.59      0.54      0.57        87\n",
      "\n",
      "    accuracy                           0.55       336\n",
      "   macro avg       0.56      0.55      0.55       336\n",
      "weighted avg       0.56      0.55      0.55       336\n",
      "\n",
      "\n",
      "Classification report for RandomForestClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68       125\n",
      "           1       0.51      0.66      0.57       124\n",
      "           2       0.73      0.46      0.56        87\n",
      "\n",
      "    accuracy                           0.61       336\n",
      "   macro avg       0.64      0.60      0.61       336\n",
      "weighted avg       0.63      0.61      0.61       336\n",
      "\n",
      "\n",
      "Classification report for SVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68       125\n",
      "           1       0.52      0.61      0.56       124\n",
      "           2       0.71      0.59      0.64        87\n",
      "\n",
      "    accuracy                           0.62       336\n",
      "   macro avg       0.64      0.62      0.63       336\n",
      "weighted avg       0.64      0.62      0.63       336\n",
      "\n",
      "\n",
      "Classification report for GradientBoostingClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.65      0.66       125\n",
      "           1       0.50      0.61      0.55       124\n",
      "           2       0.65      0.48      0.55        87\n",
      "\n",
      "    accuracy                           0.59       336\n",
      "   macro avg       0.61      0.58      0.59       336\n",
      "weighted avg       0.61      0.59      0.59       336\n",
      "\n",
      "\n",
      "Classification report for KNeighborsClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.53      0.59       125\n",
      "           1       0.45      0.51      0.48       124\n",
      "           2       0.55      0.61      0.58        87\n",
      "\n",
      "    accuracy                           0.54       336\n",
      "   macro avg       0.55      0.55      0.55       336\n",
      "weighted avg       0.55      0.54      0.54       336\n",
      "\n",
      "\n",
      "Classification report for DecisionTreeClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.49      0.50       125\n",
      "           1       0.43      0.48      0.45       124\n",
      "           2       0.53      0.49      0.51        87\n",
      "\n",
      "    accuracy                           0.49       336\n",
      "   macro avg       0.49      0.49      0.49       336\n",
      "weighted avg       0.49      0.49      0.49       336\n",
      "\n",
      "\n",
      "Classification report for GaussianNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.44      0.50       125\n",
      "           1       0.41      0.37      0.39       124\n",
      "           2       0.39      0.57      0.47        87\n",
      "\n",
      "    accuracy                           0.45       336\n",
      "   macro avg       0.46      0.46      0.45       336\n",
      "weighted avg       0.47      0.45      0.45       336\n",
      "\n",
      "\n",
      "Classification report for XGBClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.65      0.67       125\n",
      "           1       0.50      0.61      0.55       124\n",
      "           2       0.66      0.51      0.57        87\n",
      "\n",
      "    accuracy                           0.60       336\n",
      "   macro avg       0.62      0.59      0.60       336\n",
      "weighted avg       0.61      0.60      0.60       336\n",
      "\n",
      "\n",
      "Classification report for AdaBoostClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.46      0.55       125\n",
      "           1       0.47      0.72      0.57       124\n",
      "           2       0.72      0.51      0.59        87\n",
      "\n",
      "    accuracy                           0.57       336\n",
      "   macro avg       0.62      0.56      0.57       336\n",
      "weighted avg       0.61      0.57      0.57       336\n",
      "\n",
      "\n",
      "Classification report for CatBoostClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.61      0.64       125\n",
      "           1       0.49      0.64      0.55       124\n",
      "           2       0.70      0.48      0.57        87\n",
      "\n",
      "    accuracy                           0.59       336\n",
      "   macro avg       0.62      0.58      0.59       336\n",
      "weighted avg       0.61      0.59      0.59       336\n",
      "\n",
      "\n",
      "Original number of features: 768\n",
      "Reduced number of features after PCA: 683\n",
      "Re-training LogisticRegression on PCA-transformed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for LogisticRegression with PCA: 0.5982\n",
      "Re-training RandomForestClassifier on PCA-transformed data...\n",
      "Test accuracy for RandomForestClassifier with PCA: 0.5238\n",
      "Re-training SVC on PCA-transformed data...\n",
      "Test accuracy for SVC with PCA: 0.6042\n",
      "Re-training GradientBoostingClassifier on PCA-transformed data...\n",
      "Test accuracy for GradientBoostingClassifier with PCA: 0.5387\n",
      "Re-training KNeighborsClassifier on PCA-transformed data...\n",
      "Test accuracy for KNeighborsClassifier with PCA: 0.5327\n",
      "Re-training DecisionTreeClassifier on PCA-transformed data...\n",
      "Test accuracy for DecisionTreeClassifier with PCA: 0.5149\n",
      "Re-training GaussianNB on PCA-transformed data...\n",
      "Test accuracy for GaussianNB with PCA: 0.3780\n",
      "Re-training XGBClassifier on PCA-transformed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:36:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for XGBClassifier with PCA: 0.5268\n",
      "Re-training AdaBoostClassifier on PCA-transformed data...\n",
      "Test accuracy for AdaBoostClassifier with PCA: 0.5060\n",
      "Re-training CatBoostClassifier on PCA-transformed data...\n",
      "Test accuracy for CatBoostClassifier with PCA: 0.5893\n",
      "\n",
      "Results after PCA:\n",
      "                        Model  Test Accuracy with PCA\n",
      "0          LogisticRegression                0.598214\n",
      "1      RandomForestClassifier                0.523810\n",
      "2                         SVC                0.604167\n",
      "3  GradientBoostingClassifier                0.538690\n",
      "4        KNeighborsClassifier                0.532738\n",
      "5      DecisionTreeClassifier                0.514881\n",
      "6                  GaussianNB                0.377976\n",
      "7               XGBClassifier                0.526786\n",
      "8          AdaBoostClassifier                0.505952\n",
      "9          CatBoostClassifier                0.589286\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Load the Excel data\n",
    "file_path = r\"C:\\Users\\btech\\Desktop\\ml_cse22257\\bert_embeddings (1).xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=['Class'])  # Adjust to the actual feature columns\n",
    "y = data['Class']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models and parameter distributions for RandomizedSearchCV\n",
    "models = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'model__C': uniform(1e-4, 1e4),\n",
    "            'model__penalty': ['l2'],\n",
    "            'model__solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'model__n_estimators': randint(50, 200),\n",
    "            'model__max_depth': [None, 10, 20, 30],\n",
    "            'model__min_samples_split': randint(2, 15),\n",
    "            'model__min_samples_leaf': randint(1, 10)\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'model__C': uniform(1e-3, 1e3),\n",
    "            'model__gamma': ['scale', 'auto'],\n",
    "            'model__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'model__n_estimators': randint(50, 200),\n",
    "            'model__learning_rate': uniform(0.01, 0.2),\n",
    "            'model__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'model__n_neighbors': randint(3, 20),\n",
    "            'model__weights': ['uniform', 'distance'],\n",
    "            'model__p': [1, 2]\n",
    "        }\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'model__max_depth': [None, 10, 20, 30],\n",
    "            'model__min_samples_split': randint(2, 15),\n",
    "            'model__min_samples_leaf': randint(1, 10)\n",
    "        }\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {}  # No hyperparameters for GaussianNB\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model': XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
    "        'params': {\n",
    "            'model__n_estimators': randint(50, 200),\n",
    "            'model__learning_rate': uniform(0.01, 0.2),\n",
    "            'model__max_depth': randint(3, 10)\n",
    "        }\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'model': AdaBoostClassifier(),\n",
    "        'params': {\n",
    "            'model__n_estimators': randint(50, 200),\n",
    "            'model__learning_rate': uniform(0.01, 1.0)\n",
    "        }\n",
    "    },\n",
    "    'CatBoostClassifier': {\n",
    "        'model': CatBoostClassifier(verbose=0),\n",
    "        'params': {\n",
    "            'model__iterations': randint(50, 200),\n",
    "            'model__learning_rate': uniform(0.01, 0.2),\n",
    "            'model__depth': randint(4, 10)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for each model\n",
    "results = []\n",
    "best_models = {}\n",
    "for model_name, model_info in models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Applies scaling\n",
    "        ('model', model_info['model'])\n",
    "    ])\n",
    "\n",
    "    search = RandomizedSearchCV(pipe, model_info['params'], n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best_models[model_name] = search.best_estimator_\n",
    "    train_scores = cross_val_score(search.best_estimator_, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    train_mean = np.mean(train_scores)\n",
    "    train_std = np.std(train_scores)\n",
    "\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Best Parameters': search.best_params_,\n",
    "        'Best CV Score': search.best_score_,\n",
    "        'Train Mean Accuracy': train_mean,\n",
    "        'Train Std Dev': train_std\n",
    "    })\n",
    "\n",
    "    print(f\"Best parameters for {model_name}: {search.best_params_}\")\n",
    "    print(f\"Best cross-validation score for {model_name}: {search.best_score_:.4f}\")\n",
    "    print(f\"Train mean accuracy: {train_mean:.4f}, Train std deviation: {train_std:.4f}\\n\")\n",
    "\n",
    "# Evaluate best models on the test set\n",
    "print(\"Evaluating models on test data...\")\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Classification report for {model_name}:\\n{classification_report(y_test, y_pred)}\\n\")\n",
    "\n",
    "# Apply PCA to the data\n",
    "pca = PCA(n_components=0.9999)  # Retain 99.99% of the variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "print(f\"Reduced number of features after PCA: {X_train_pca.shape[1]}\")\n",
    "\n",
    "# Re-evaluate models on PCA-transformed data\n",
    "pca_results = []\n",
    "for model_name, model_info in models.items():\n",
    "    print(f\"Re-training {model_name} on PCA-transformed data...\")\n",
    "    model = model_info['model']\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred_pca = model.predict(X_test_pca)\n",
    "    acc_pca = accuracy_score(y_test, y_pred_pca)\n",
    "    pca_results.append({\n",
    "        'Model': model_name,\n",
    "        'Test Accuracy with PCA': acc_pca\n",
    "    })\n",
    "    print(f\"Test accuracy for {model_name} with PCA: {acc_pca:.4f}\")\n",
    "\n",
    "# Display results after PCA\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "print(\"\\nResults after PCA:\")\n",
    "print(pca_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd3986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
