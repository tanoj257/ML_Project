{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5Yhq4GTQD4t",
        "outputId": "d545fa19-81ca-4225-a1a8-9dc46d72dca7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column names: Index([      0,       1,       2,       3,       4,       5,       6,       7,\n",
            "             8,       9,\n",
            "       ...\n",
            "           759,     760,     761,     762,     763,     764,     765,     766,\n",
            "           767, 'Class'],\n",
            "      dtype='object', length=769)\n",
            "Tuning LogisticRegression...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for LogisticRegression: {'model__C': 580.8362216819946, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
            "Best cross-validation score for LogisticRegression: 0.5097\n",
            "\n",
            "Tuning RandomForestClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for RandomForestClassifier: {'model__max_depth': 30, 'model__min_samples_leaf': 8, 'model__min_samples_split': 4, 'model__n_estimators': 199}\n",
            "Best cross-validation score for RandomForestClassifier: 0.5841\n",
            "\n",
            "Tuning SVC...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for SVC: {'model__C': 20.585494295802448, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\n",
            "Best cross-validation score for SVC: 0.5588\n",
            "\n",
            "Tuning GradientBoostingClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for GradientBoostingClassifier: {'model__learning_rate': 0.11495128632644756, 'model__max_depth': 3, 'model__n_estimators': 98}\n",
            "Best cross-validation score for GradientBoostingClassifier: 0.5543\n",
            "\n",
            "Tuning KNeighborsClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for KNeighborsClassifier: {'model__n_neighbors': 14, 'model__p': 1, 'model__weights': 'uniform'}\n",
            "Best cross-validation score for KNeighborsClassifier: 0.5774\n",
            "\n",
            "Tuning DecisionTreeClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for DecisionTreeClassifier: {'model__max_depth': 10, 'model__min_samples_leaf': 6, 'model__min_samples_split': 3}\n",
            "Best cross-validation score for DecisionTreeClassifier: 0.4844\n",
            "\n",
            "Tuning GaussianNB...\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for GaussianNB: {}\n",
            "Best cross-validation score for GaussianNB: 0.5372\n",
            "\n",
            "Tuning XGBClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:46:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for XGBClassifier: {'model__learning_rate': 0.13349630192554332, 'model__max_depth': 4, 'model__n_estimators': 71}\n",
            "Best cross-validation score for XGBClassifier: 0.5484\n",
            "\n",
            "Tuning LGBMClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017811 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 195821\n",
            "[LightGBM] [Info] Number of data points in the train set: 1344, number of used features: 768\n",
            "[LightGBM] [Info] Start training from score -1.094158\n",
            "[LightGBM] [Info] Start training from score -0.913690\n",
            "[LightGBM] [Info] Start training from score -1.331288\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best parameters for LGBMClassifier: {'model__learning_rate': 0.10121399684340719, 'model__max_depth': 20, 'model__n_estimators': 100}\n",
            "Best cross-validation score for LGBMClassifier: 0.5565\n",
            "\n",
            "Tuning CatBoostClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for CatBoostClassifier: {'model__depth': 4, 'model__iterations': 98, 'model__learning_rate': 0.11495493205167782}\n",
            "Best cross-validation score for CatBoostClassifier: 0.5781\n",
            "\n",
            "Evaluating LogisticRegression on the test set...\n",
            "Classification report for LogisticRegression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.55      0.60       125\n",
            "           1       0.45      0.56      0.50       124\n",
            "           2       0.59      0.53      0.56        87\n",
            "\n",
            "    accuracy                           0.55       336\n",
            "   macro avg       0.57      0.55      0.55       336\n",
            "weighted avg       0.57      0.55      0.55       336\n",
            "\n",
            "\n",
            "Evaluating RandomForestClassifier on the test set...\n",
            "Classification report for RandomForestClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.62      0.65       125\n",
            "           1       0.48      0.65      0.55       124\n",
            "           2       0.68      0.45      0.54        87\n",
            "\n",
            "    accuracy                           0.58       336\n",
            "   macro avg       0.62      0.57      0.58       336\n",
            "weighted avg       0.61      0.58      0.59       336\n",
            "\n",
            "\n",
            "Evaluating SVC on the test set...\n",
            "Classification report for SVC:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.63      0.64       125\n",
            "           1       0.48      0.59      0.53       124\n",
            "           2       0.69      0.52      0.59        87\n",
            "\n",
            "    accuracy                           0.59       336\n",
            "   macro avg       0.61      0.58      0.59       336\n",
            "weighted avg       0.60      0.59      0.59       336\n",
            "\n",
            "\n",
            "Evaluating GradientBoostingClassifier on the test set...\n",
            "Classification report for GradientBoostingClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.65      0.66       125\n",
            "           1       0.49      0.60      0.54       124\n",
            "           2       0.66      0.48      0.56        87\n",
            "\n",
            "    accuracy                           0.59       336\n",
            "   macro avg       0.61      0.58      0.59       336\n",
            "weighted avg       0.60      0.59      0.59       336\n",
            "\n",
            "\n",
            "Evaluating KNeighborsClassifier on the test set...\n",
            "Classification report for KNeighborsClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.66      0.66       125\n",
            "           1       0.49      0.56      0.52       124\n",
            "           2       0.62      0.51      0.56        87\n",
            "\n",
            "    accuracy                           0.58       336\n",
            "   macro avg       0.59      0.57      0.58       336\n",
            "weighted avg       0.59      0.58      0.58       336\n",
            "\n",
            "\n",
            "Evaluating DecisionTreeClassifier on the test set...\n",
            "Classification report for DecisionTreeClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.53      0.58       125\n",
            "           1       0.45      0.54      0.49       124\n",
            "           2       0.54      0.51      0.52        87\n",
            "\n",
            "    accuracy                           0.53       336\n",
            "   macro avg       0.54      0.52      0.53       336\n",
            "weighted avg       0.54      0.53      0.53       336\n",
            "\n",
            "\n",
            "Evaluating GaussianNB on the test set...\n",
            "Classification report for GaussianNB:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.64      0.68       125\n",
            "           1       0.53      0.47      0.50       124\n",
            "           2       0.48      0.63      0.55        87\n",
            "\n",
            "    accuracy                           0.57       336\n",
            "   macro avg       0.57      0.58      0.57       336\n",
            "weighted avg       0.59      0.57      0.58       336\n",
            "\n",
            "\n",
            "Evaluating XGBClassifier on the test set...\n",
            "Classification report for XGBClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.66      0.67       125\n",
            "           1       0.48      0.61      0.54       124\n",
            "           2       0.67      0.46      0.54        87\n",
            "\n",
            "    accuracy                           0.59       336\n",
            "   macro avg       0.61      0.58      0.59       336\n",
            "weighted avg       0.61      0.59      0.59       336\n",
            "\n",
            "\n",
            "Evaluating LGBMClassifier on the test set...\n",
            "Classification report for LGBMClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.64      0.66       125\n",
            "           1       0.48      0.62      0.54       124\n",
            "           2       0.66      0.46      0.54        87\n",
            "\n",
            "    accuracy                           0.59       336\n",
            "   macro avg       0.61      0.57      0.58       336\n",
            "weighted avg       0.61      0.59      0.59       336\n",
            "\n",
            "\n",
            "Evaluating CatBoostClassifier on the test set...\n",
            "Classification report for CatBoostClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.64      0.66       125\n",
            "           1       0.47      0.60      0.53       124\n",
            "           2       0.67      0.47      0.55        87\n",
            "\n",
            "    accuracy                           0.58       336\n",
            "   macro avg       0.61      0.57      0.58       336\n",
            "weighted avg       0.60      0.58      0.59       336\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import numpy as np\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Load the Excel data\n",
        "file_path = r\"/content/roberta_embeddings exal (1).xlsx\"\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Display column names to confirm structure\n",
        "print(\"Column names:\", data.columns)\n",
        "\n",
        "# Define features and target\n",
        "X = data.drop(columns=['Class'])  # Adjust to the actual feature columns\n",
        "y = data['Class']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the models and parameter distributions for RandomizedSearchCV\n",
        "models = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(),\n",
        "        'params': {\n",
        "            'model__C': uniform(1e-4, 1e4),\n",
        "            'model__penalty': ['l2'],\n",
        "            'model__solver': ['liblinear']\n",
        "        }\n",
        "    },\n",
        "    'RandomForestClassifier': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': randint(50, 200),\n",
        "            'model__max_depth': [None, 10, 20, 30],\n",
        "            'model__min_samples_split': randint(2, 15),\n",
        "            'model__min_samples_leaf': randint(1, 10)\n",
        "        }\n",
        "    },\n",
        "    'SVC': {\n",
        "        'model': SVC(),\n",
        "        'params': {\n",
        "            'model__C': uniform(1e-3, 1e3),\n",
        "            'model__gamma': ['scale', 'auto'],\n",
        "            'model__kernel': ['linear', 'rbf']\n",
        "        }\n",
        "    },\n",
        "    'GradientBoostingClassifier': {\n",
        "        'model': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': randint(50, 200),\n",
        "            'model__learning_rate': uniform(0.01, 0.2),\n",
        "            'model__max_depth': [3, 5, 7]\n",
        "        }\n",
        "    },\n",
        "    'KNeighborsClassifier': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'model__n_neighbors': randint(3, 20),\n",
        "            'model__weights': ['uniform', 'distance'],\n",
        "            'model__p': [1, 2]\n",
        "        }\n",
        "    },\n",
        "    'DecisionTreeClassifier': {\n",
        "        'model': DecisionTreeClassifier(),\n",
        "        'params': {\n",
        "            'model__max_depth': [None, 10, 20, 30],\n",
        "            'model__min_samples_split': randint(2, 15),\n",
        "            'model__min_samples_leaf': randint(1, 10)\n",
        "        }\n",
        "    },\n",
        "    'GaussianNB': {\n",
        "        'model': GaussianNB(),\n",
        "        'params': {}  # No hyperparameters for GaussianNB\n",
        "    },\n",
        "    'XGBClassifier': {\n",
        "        'model': XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
        "        'params': {\n",
        "            'model__n_estimators': randint(50, 200),\n",
        "            'model__learning_rate': uniform(0.01, 0.2),\n",
        "            'model__max_depth': randint(3, 10)\n",
        "        }\n",
        "    },\n",
        "    'LGBMClassifier': {\n",
        "        'model': LGBMClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': randint(50, 200),\n",
        "            'model__learning_rate': uniform(0.01, 0.2),\n",
        "            'model__max_depth': [-1, 10, 20]\n",
        "        }\n",
        "    },\n",
        "    'CatBoostClassifier': {\n",
        "        'model': CatBoostClassifier(verbose=0),\n",
        "        'params': {\n",
        "            'model__iterations': randint(50, 200),\n",
        "            'model__learning_rate': uniform(0.01, 0.2),\n",
        "            'model__depth': randint(4, 10)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for each model\n",
        "best_models = {}\n",
        "for model_name, model_info in models.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),  # Applies scaling\n",
        "        ('model', model_info['model'])\n",
        "    ])\n",
        "\n",
        "    search = RandomizedSearchCV(pipe, model_info['params'], n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    best_models[model_name] = search.best_estimator_\n",
        "    print(f\"Best parameters for {model_name}: {search.best_params_}\")\n",
        "    print(f\"Best cross-validation score for {model_name}: {search.best_score_:.4f}\\n\")\n",
        "\n",
        "# Evaluate best models on the test set\n",
        "for model_name, model in best_models.items():\n",
        "    print(f\"Evaluating {model_name} on the test set...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Classification report for {model_name}:\\n{classification_report(y_test, y_pred)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# Load the CSV data\n",
        "file_path = r\"/content/training_with_glove embeddings_split.xlsx\"\n",
        "data = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "# Define features and target\n",
        "X = data.drop(columns=['input', 'Class'])\n",
        "y = data['Class']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the models and parameter grids\n",
        "models = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(),\n",
        "        'params': {\n",
        "            'model__C': np.logspace(-4, 4, 10),\n",
        "            'model__penalty': ['l1', 'l2'],\n",
        "            'model__solver': ['liblinear', 'saga']\n",
        "        }\n",
        "    },\n",
        "    'RandomForestClassifier': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': [50, 100, 200],\n",
        "            'model__max_depth': [None, 10, 20, 30],\n",
        "            'model__min_samples_split': [2, 5, 10],\n",
        "            'model__min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    },\n",
        "    'SVC': {\n",
        "        'model': SVC(),\n",
        "        'params': {\n",
        "            'model__C': np.logspace(-3, 3, 7),\n",
        "            'model__gamma': ['scale', 'auto'],\n",
        "            'model__kernel': ['linear', 'rbf']\n",
        "        }\n",
        "    },\n",
        "    'KNeighborsClassifier': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'model__n_neighbors': range(3, 15),\n",
        "            'model__weights': ['uniform', 'distance'],\n",
        "            'model__metric': ['euclidean', 'manhattan']\n",
        "        }\n",
        "    },\n",
        "    'GradientBoostingClassifier': {\n",
        "        'model': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': [50, 100, 200],\n",
        "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'model__max_depth': [3, 5, 7]\n",
        "        }\n",
        "    },\n",
        "    'DecisionTreeClassifier': {\n",
        "        'model': DecisionTreeClassifier(),\n",
        "        'params': {\n",
        "            'model__max_depth': [None, 10, 20, 30],\n",
        "            'model__min_samples_split': [2, 5, 10],\n",
        "            'model__min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    },\n",
        "    'XGBClassifier': {\n",
        "        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'model__n_estimators': [50, 100, 200],\n",
        "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'model__max_depth': [3, 5, 7]\n",
        "        }\n",
        "    },\n",
        "    'CatBoostClassifier': {\n",
        "        'model': CatBoostClassifier(verbose=0),\n",
        "        'params': {\n",
        "            'model__iterations': [100, 200, 300],\n",
        "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'model__depth': [3, 5, 7]\n",
        "        }\n",
        "    },\n",
        "    'LGBMClassifier': {\n",
        "        'model': LGBMClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': [50, 100, 200],\n",
        "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'model__max_depth': [-1, 10, 20]\n",
        "        }\n",
        "    },\n",
        "    'GaussianNB': {\n",
        "        'model': GaussianNB(),\n",
        "        'params': {\n",
        "            # GaussianNB has no hyperparameters to tune\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for each model\n",
        "best_models = {}\n",
        "for model_name, model_info in models.items():\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', model_info['model'])\n",
        "    ])\n",
        "\n",
        "    if model_info['params']:\n",
        "        search = RandomizedSearchCV(pipe, model_info['params'], n_iter=10, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_models[model_name] = search.best_estimator_\n",
        "        print(f\"Best parameters for {model_name}: {search.best_params_}\")\n",
        "        print(f\"Best cross-validation score for {model_name}: {search.best_score_:.4f}\\n\")\n",
        "    else:\n",
        "        # Directly fit if no hyperparameters to tune\n",
        "        pipe.fit(X_train, y_train)\n",
        "        best_models[model_name] = pipe\n",
        "\n",
        "# Evaluate best models on the test set\n",
        "for model_name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Classification report for {model_name}:\\n{classification_report(y_test, y_pred)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMQ9CqZPsfPb",
        "outputId": "c2fa7e2b-646e-49a0-f85a-58220e219a72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for LogisticRegression: {'model__solver': 'saga', 'model__penalty': 'l2', 'model__C': 0.3593813663804626}\n",
            "Best cross-validation score for LogisticRegression: 0.5052\n",
            "\n",
            "Best parameters for RandomForestClassifier: {'model__n_estimators': 50, 'model__min_samples_split': 2, 'model__min_samples_leaf': 4, 'model__max_depth': 10}\n",
            "Best cross-validation score for RandomForestClassifier: 0.5164\n",
            "\n",
            "Best parameters for SVC: {'model__kernel': 'rbf', 'model__gamma': 'scale', 'model__C': 1.0}\n",
            "Best cross-validation score for SVC: 0.5246\n",
            "\n",
            "Best parameters for KNeighborsClassifier: {'model__weights': 'distance', 'model__n_neighbors': 4, 'model__metric': 'manhattan'}\n",
            "Best cross-validation score for KNeighborsClassifier: 0.4785\n",
            "\n",
            "Best parameters for GradientBoostingClassifier: {'model__n_estimators': 50, 'model__max_depth': 5, 'model__learning_rate': 0.2}\n",
            "Best cross-validation score for GradientBoostingClassifier: 0.5253\n",
            "\n",
            "Best parameters for DecisionTreeClassifier: {'model__min_samples_split': 5, 'model__min_samples_leaf': 2, 'model__max_depth': 10}\n",
            "Best cross-validation score for DecisionTreeClassifier: 0.4903\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [21:20:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for XGBClassifier: {'model__n_estimators': 100, 'model__max_depth': 5, 'model__learning_rate': 0.1}\n",
            "Best cross-validation score for XGBClassifier: 0.5119\n",
            "\n",
            "Best parameters for CatBoostClassifier: {'model__learning_rate': 0.1, 'model__iterations': 200, 'model__depth': 5}\n",
            "Best cross-validation score for CatBoostClassifier: 0.5238\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003451 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 76456\n",
            "[LightGBM] [Info] Number of data points in the train set: 1344, number of used features: 300\n",
            "[LightGBM] [Info] Start training from score -1.094158\n",
            "[LightGBM] [Info] Start training from score -0.913690\n",
            "[LightGBM] [Info] Start training from score -1.331288\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best parameters for LGBMClassifier: {'model__n_estimators': 100, 'model__max_depth': 10, 'model__learning_rate': 0.1}\n",
            "Best cross-validation score for LGBMClassifier: 0.5164\n",
            "\n",
            "Classification report for LogisticRegression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.44      0.51       125\n",
            "           1       0.45      0.65      0.53       124\n",
            "           2       0.48      0.36      0.41        87\n",
            "\n",
            "    accuracy                           0.50       336\n",
            "   macro avg       0.51      0.48      0.48       336\n",
            "weighted avg       0.51      0.50      0.49       336\n",
            "\n",
            "\n",
            "Classification report for RandomForestClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.50      0.55       125\n",
            "           1       0.44      0.61      0.51       124\n",
            "           2       0.64      0.43      0.51        87\n",
            "\n",
            "    accuracy                           0.52       336\n",
            "   macro avg       0.56      0.51      0.52       336\n",
            "weighted avg       0.55      0.52      0.52       336\n",
            "\n",
            "\n",
            "Classification report for SVC:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.42      0.50       125\n",
            "           1       0.42      0.66      0.51       124\n",
            "           2       0.58      0.39      0.47        87\n",
            "\n",
            "    accuracy                           0.50       336\n",
            "   macro avg       0.55      0.49      0.49       336\n",
            "weighted avg       0.54      0.50      0.50       336\n",
            "\n",
            "\n",
            "Classification report for KNeighborsClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.45      0.51       125\n",
            "           1       0.42      0.48      0.45       124\n",
            "           2       0.47      0.54      0.50        87\n",
            "\n",
            "    accuracy                           0.49       336\n",
            "   macro avg       0.50      0.49      0.49       336\n",
            "weighted avg       0.50      0.49      0.49       336\n",
            "\n",
            "\n",
            "Classification report for GradientBoostingClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.52      0.55       125\n",
            "           1       0.46      0.58      0.52       124\n",
            "           2       0.61      0.48      0.54        87\n",
            "\n",
            "    accuracy                           0.53       336\n",
            "   macro avg       0.55      0.53      0.53       336\n",
            "weighted avg       0.54      0.53      0.53       336\n",
            "\n",
            "\n",
            "Classification report for DecisionTreeClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.47      0.49       125\n",
            "           1       0.40      0.42      0.41       124\n",
            "           2       0.46      0.47      0.47        87\n",
            "\n",
            "    accuracy                           0.45       336\n",
            "   macro avg       0.45      0.45      0.45       336\n",
            "weighted avg       0.45      0.45      0.45       336\n",
            "\n",
            "\n",
            "Classification report for XGBClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.50      0.53       125\n",
            "           1       0.45      0.57      0.50       124\n",
            "           2       0.64      0.49      0.56        87\n",
            "\n",
            "    accuracy                           0.53       336\n",
            "   macro avg       0.55      0.52      0.53       336\n",
            "weighted avg       0.54      0.53      0.53       336\n",
            "\n",
            "\n",
            "Classification report for CatBoostClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.49      0.53       125\n",
            "           1       0.46      0.61      0.53       124\n",
            "           2       0.68      0.51      0.58        87\n",
            "\n",
            "    accuracy                           0.54       336\n",
            "   macro avg       0.57      0.54      0.54       336\n",
            "weighted avg       0.56      0.54      0.54       336\n",
            "\n",
            "\n",
            "Classification report for LGBMClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.51      0.54       125\n",
            "           1       0.45      0.59      0.51       124\n",
            "           2       0.62      0.45      0.52        87\n",
            "\n",
            "    accuracy                           0.52       336\n",
            "   macro avg       0.55      0.52      0.52       336\n",
            "weighted avg       0.54      0.52      0.52       336\n",
            "\n",
            "\n",
            "Classification report for GaussianNB:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.38      0.44       125\n",
            "           1       0.39      0.29      0.33       124\n",
            "           2       0.32      0.57      0.41        87\n",
            "\n",
            "    accuracy                           0.40       336\n",
            "   macro avg       0.41      0.41      0.40       336\n",
            "weighted avg       0.42      0.40      0.39       336\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "import numpy as np\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Load the Excel data\n",
        "file_path = r\"/content/bert_embeddings (1).xlsx\"\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Display column names to confirm structure\n",
        "print(\"Column names:\", data.columns)\n",
        "\n",
        "# Define features and target\n",
        "X = data.drop(columns=['Class'])  # Adjust to the actual feature columns\n",
        "y = data['Class']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the models and parameter distributions for RandomizedSearchCV\n",
        "models = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(),\n",
        "        'params': {\n",
        "            'model__C': uniform(1e-4, 1e4),\n",
        "            'model__penalty': ['l2'],\n",
        "            'model__solver': ['liblinear']\n",
        "        }\n",
        "    },\n",
        "    'RandomForestClassifier': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': randint(50, 200),\n",
        "            'model__max_depth': [None, 10, 20, 30],\n",
        "            'model__min_samples_split': randint(2, 15),\n",
        "            'model__min_samples_leaf': randint(1, 10)\n",
        "        }\n",
        "    },\n",
        "    'SVC': {\n",
        "        'model': SVC(),\n",
        "        'params': {\n",
        "            'model__C': uniform(1e-3, 1e3),\n",
        "            'model__gamma': ['scale', 'auto'],\n",
        "            'model__kernel': ['linear', 'rbf']\n",
        "        }\n",
        "    },\n",
        "    'GradientBoostingClassifier': {\n",
        "        'model': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': randint(50, 200),\n",
        "            'model__learning_rate': uniform(0.01, 0.2),\n",
        "            'model__max_depth': [3, 5, 7]\n",
        "        }\n",
        "    },\n",
        "    'KNeighborsClassifier': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'model__n_neighbors': randint(3, 20),\n",
        "            'model__weights': ['uniform', 'distance'],\n",
        "            'model__p': [1, 2]\n",
        "        }\n",
        "    },\n",
        "    'DecisionTreeClassifier': {\n",
        "        'model': DecisionTreeClassifier(),\n",
        "        'params': {\n",
        "            'model__max_depth': [None, 10, 20, 30],\n",
        "            'model__min_samples_split': randint(2, 15),\n",
        "            'model__min_samples_leaf': randint(1, 10)\n",
        "        }\n",
        "    },\n",
        "    'GaussianNB': {\n",
        "        'model': GaussianNB(),\n",
        "        'params': {}  # No hyperparameters for GaussianNB\n",
        "    },\n",
        "    'XGBClassifier': {\n",
        "        'model': XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
        "        'params': {\n",
        "            'model__n_estimators': randint(50, 200),\n",
        "            'model__learning_rate': uniform(0.01, 0.2),\n",
        "            'model__max_depth': randint(3, 10)\n",
        "        }\n",
        "    },\n",
        "    'LGBMClassifier': {\n",
        "        'model': LGBMClassifier(),\n",
        "        'params': {\n",
        "            'model__n_estimators': randint(50, 200),\n",
        "            'model__learning_rate': uniform(0.01, 0.2),\n",
        "            'model__max_depth': [-1, 10, 20]\n",
        "        }\n",
        "    },\n",
        "    'CatBoostClassifier': {\n",
        "        'model': CatBoostClassifier(verbose=0),\n",
        "        'params': {\n",
        "            'model__iterations': randint(50, 200),\n",
        "            'model__learning_rate': uniform(0.01, 0.2),\n",
        "            'model__depth': randint(4, 10)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for each model\n",
        "best_models = {}\n",
        "for model_name, model_info in models.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),  # Applies scaling\n",
        "        ('model', model_info['model'])\n",
        "    ])\n",
        "\n",
        "    search = RandomizedSearchCV(pipe, model_info['params'], n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    best_models[model_name] = search.best_estimator_\n",
        "    print(f\"Best parameters for {model_name}: {search.best_params_}\")\n",
        "    print(f\"Best cross-validation score for {model_name}: {search.best_score_:.4f}\\n\")\n",
        "\n",
        "# Evaluate best models on the test set\n",
        "for model_name, model in best_models.items():\n",
        "    print(f\"Evaluating {model_name} on the test set...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Classification report for {model_name}:\\n{classification_report(y_test, y_pred)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC4UmhDDjHuS",
        "outputId": "5a3b6b8b-1da2-4ebe-fb3d-f61982ce8927"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names: Index([      0,       1,       2,       3,       4,       5,       6,       7,\n",
            "             8,       9,\n",
            "       ...\n",
            "           759,     760,     761,     762,     763,     764,     765,     766,\n",
            "           767, 'Class'],\n",
            "      dtype='object', length=769)\n",
            "Tuning LogisticRegression...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for LogisticRegression: {'model__C': 1559.9453033620264, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
            "Best cross-validation score for LogisticRegression: 0.5216\n",
            "\n",
            "Tuning RandomForestClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for RandomForestClassifier: {'model__max_depth': None, 'model__min_samples_leaf': 2, 'model__min_samples_split': 9, 'model__n_estimators': 87}\n",
            "Best cross-validation score for RandomForestClassifier: 0.5484\n",
            "\n",
            "Tuning SVC...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for SVC: {'model__C': 20.585494295802448, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\n",
            "Best cross-validation score for SVC: 0.5677\n",
            "\n",
            "Tuning GradientBoostingClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for GradientBoostingClassifier: {'model__learning_rate': 0.038573363584388155, 'model__max_depth': 7, 'model__n_estimators': 199}\n",
            "Best cross-validation score for GradientBoostingClassifier: 0.5409\n",
            "\n",
            "Tuning KNeighborsClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for KNeighborsClassifier: {'model__n_neighbors': 5, 'model__p': 2, 'model__weights': 'uniform'}\n",
            "Best cross-validation score for KNeighborsClassifier: 0.5536\n",
            "\n",
            "Tuning DecisionTreeClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for DecisionTreeClassifier: {'model__max_depth': 10, 'model__min_samples_leaf': 8, 'model__min_samples_split': 13}\n",
            "Best cross-validation score for DecisionTreeClassifier: 0.4888\n",
            "\n",
            "Tuning GaussianNB...\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:320: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for GaussianNB: {}\n",
            "Best cross-validation score for GaussianNB: 0.4881\n",
            "\n",
            "Tuning XGBClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [23:17:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for XGBClassifier: {'model__learning_rate': 0.13349630192554332, 'model__max_depth': 4, 'model__n_estimators': 71}\n",
            "Best cross-validation score for XGBClassifier: 0.5536\n",
            "\n",
            "Tuning LGBMClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011488 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 195795\n",
            "[LightGBM] [Info] Number of data points in the train set: 1344, number of used features: 768\n",
            "[LightGBM] [Info] Start training from score -1.094158\n",
            "[LightGBM] [Info] Start training from score -0.913690\n",
            "[LightGBM] [Info] Start training from score -1.331288\n",
            "Best parameters for LGBMClassifier: {'model__learning_rate': 0.15639878836228102, 'model__max_depth': -1, 'model__n_estimators': 70}\n",
            "Best cross-validation score for LGBMClassifier: 0.5670\n",
            "\n",
            "Tuning CatBoostClassifier...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Best parameters for CatBoostClassifier: {'model__depth': 7, 'model__iterations': 142, 'model__learning_rate': 0.04668695797323276}\n",
            "Best cross-validation score for CatBoostClassifier: 0.5536\n",
            "\n",
            "Evaluating LogisticRegression on the test set...\n",
            "Classification report for LogisticRegression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.58      0.59       125\n",
            "           1       0.48      0.53      0.51       124\n",
            "           2       0.59      0.54      0.57        87\n",
            "\n",
            "    accuracy                           0.55       336\n",
            "   macro avg       0.56      0.55      0.55       336\n",
            "weighted avg       0.56      0.55      0.55       336\n",
            "\n",
            "\n",
            "Evaluating RandomForestClassifier on the test set...\n",
            "Classification report for RandomForestClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.64      0.65       125\n",
            "           1       0.51      0.62      0.56       124\n",
            "           2       0.70      0.52      0.60        87\n",
            "\n",
            "    accuracy                           0.60       336\n",
            "   macro avg       0.63      0.59      0.60       336\n",
            "weighted avg       0.62      0.60      0.60       336\n",
            "\n",
            "\n",
            "Evaluating SVC on the test set...\n",
            "Classification report for SVC:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.66      0.68       125\n",
            "           1       0.52      0.61      0.56       124\n",
            "           2       0.71      0.59      0.64        87\n",
            "\n",
            "    accuracy                           0.62       336\n",
            "   macro avg       0.64      0.62      0.63       336\n",
            "weighted avg       0.64      0.62      0.63       336\n",
            "\n",
            "\n",
            "Evaluating GradientBoostingClassifier on the test set...\n",
            "Classification report for GradientBoostingClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.64      0.66       125\n",
            "           1       0.52      0.64      0.57       124\n",
            "           2       0.72      0.53      0.61        87\n",
            "\n",
            "    accuracy                           0.61       336\n",
            "   macro avg       0.64      0.60      0.61       336\n",
            "weighted avg       0.63      0.61      0.61       336\n",
            "\n",
            "\n",
            "Evaluating KNeighborsClassifier on the test set...\n",
            "Classification report for KNeighborsClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.53      0.59       125\n",
            "           1       0.45      0.51      0.48       124\n",
            "           2       0.55      0.61      0.58        87\n",
            "\n",
            "    accuracy                           0.54       336\n",
            "   macro avg       0.55      0.55      0.55       336\n",
            "weighted avg       0.55      0.54      0.54       336\n",
            "\n",
            "\n",
            "Evaluating DecisionTreeClassifier on the test set...\n",
            "Classification report for DecisionTreeClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.50      0.50       125\n",
            "           1       0.43      0.46      0.44       124\n",
            "           2       0.53      0.46      0.49        87\n",
            "\n",
            "    accuracy                           0.48       336\n",
            "   macro avg       0.48      0.47      0.48       336\n",
            "weighted avg       0.48      0.48      0.48       336\n",
            "\n",
            "\n",
            "Evaluating GaussianNB on the test set...\n",
            "Classification report for GaussianNB:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.44      0.50       125\n",
            "           1       0.41      0.37      0.39       124\n",
            "           2       0.39      0.57      0.47        87\n",
            "\n",
            "    accuracy                           0.45       336\n",
            "   macro avg       0.46      0.46      0.45       336\n",
            "weighted avg       0.47      0.45      0.45       336\n",
            "\n",
            "\n",
            "Evaluating XGBClassifier on the test set...\n",
            "Classification report for XGBClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.65      0.67       125\n",
            "           1       0.50      0.61      0.55       124\n",
            "           2       0.66      0.51      0.57        87\n",
            "\n",
            "    accuracy                           0.60       336\n",
            "   macro avg       0.62      0.59      0.60       336\n",
            "weighted avg       0.61      0.60      0.60       336\n",
            "\n",
            "\n",
            "Evaluating LGBMClassifier on the test set...\n",
            "Classification report for LGBMClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.66      0.67       125\n",
            "           1       0.52      0.64      0.57       124\n",
            "           2       0.73      0.53      0.61        87\n",
            "\n",
            "    accuracy                           0.62       336\n",
            "   macro avg       0.64      0.61      0.62       336\n",
            "weighted avg       0.63      0.62      0.62       336\n",
            "\n",
            "\n",
            "Evaluating CatBoostClassifier on the test set...\n",
            "Classification report for CatBoostClassifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.61      0.64       125\n",
            "           1       0.49      0.64      0.55       124\n",
            "           2       0.70      0.48      0.57        87\n",
            "\n",
            "    accuracy                           0.59       336\n",
            "   macro avg       0.62      0.58      0.59       336\n",
            "weighted avg       0.61      0.59      0.59       336\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUUkRBh0OvGZ",
        "outputId": "3c2056d3-ff01-44e5-be1e-cd438eca6745"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chardet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfXvKFjQMKKy",
        "outputId": "c57ad9fb-9e02-43fd-ae8f-9d776e1f0735"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9bxA45ztMy1",
        "outputId": "7f043ba1-f07f-4f38-deb6-f370c4b3fd7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the GloVe file to UTF-8 encoding\n",
        "input_path = \"/content/training_with_glove embeddings_split.xlsx\"  # Original GloVe file\n",
        "output_path = \"/content/training_with_glove embeddings_split.xlsx\"  # Output UTF-8 file\n",
        "\n",
        "with open(input_path, \"r\", encoding=\"latin1\") as infile:\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        for line in infile:\n",
        "            outfile.write(line)\n",
        "\n",
        "print(f\"File converted and saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_ljy6k0MR5a",
        "outputId": "0a47a49d-5643-4d36-c73c-29e2adf6190c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File converted and saved to /content/training_with_glove embeddings_split.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "!pip install dask[dataframe]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT6gADseQ9VY",
        "outputId": "55655440-1fe6-49db-c5c0-54dd2c936371"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.5.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.2.2)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (17.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.21.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.16.0)\n",
            "Downloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dask-expr\n",
            "Successfully installed dask-expr-1.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zw-cDP8xRAfh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}